{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95633d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12be2743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 07:32:11.689559: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-01 07:32:12.406712: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-01 07:32:12.541715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-01 07:32:12.541771: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-01 07:32:14.399704: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-01 07:32:14.399920: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-01 07:32:14.399937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-01 07:32:16.625951: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-01 07:32:16.626738: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-01 07:32:16.626817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lm-dep-74fd78c88-wp8c6): /proc/driver/nvidia/version does not exist\n",
      "2023-03-01 07:32:16.629772: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import stellargraph as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b80efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_structure = ('source user@domain', 'source computer', 'destination computer')\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1aa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv('../lm-vol/LANL_train_unique_v2.csv', chunksize=1000000):\n",
    "    for index, row in chunk.iterrows():\n",
    "        user_node = row[graph_structure[0]]\n",
    "        ip_node = row[graph_structure[1]]\n",
    "        service_node = row[graph_structure[2]]\n",
    "        G.add_nodes_from([ip_node, service_node, user_node])\n",
    "        G.add_edge(ip_node, user_node)\n",
    "        G.add_edge(ip_node, service_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2224317",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, \"../lm-vol/LANL_train_v2.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63b7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "graph = StellarGraph.from_networkx(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167c84b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 92127, Edges: 981716\n",
      "\n",
      " Node types:\n",
      "  default: [92127]\n",
      "    Features: none\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [981716]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "print(graph.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da94a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1.0\n",
    "q = 1.0\n",
    "dimensions = 128\n",
    "num_walks = 20\n",
    "walk_length = 10\n",
    "window_size = 5\n",
    "num_iter = 100\n",
    "workers = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4275c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import BiasedRandomWalk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# init callback class\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff461daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of random walks for Graph: 1842540\n",
      "Loss after epoch 0: 1158454.875\n",
      "Loss after epoch 1: 924504.375\n",
      "Loss after epoch 2: 806799.75\n",
      "Loss after epoch 3: 718559.5\n",
      "Loss after epoch 4: 679811.5\n",
      "Loss after epoch 5: 599827.5\n",
      "Loss after epoch 6: 629410.0\n",
      "Loss after epoch 7: 601714.0\n",
      "Loss after epoch 8: 596464.0\n",
      "Loss after epoch 9: 591978.5\n",
      "Loss after epoch 10: 600476.5\n",
      "Loss after epoch 11: 593272.5\n",
      "Loss after epoch 12: 534011.0\n",
      "Loss after epoch 13: 525481.0\n",
      "Loss after epoch 14: 523690.0\n",
      "Loss after epoch 15: 544804.0\n",
      "Loss after epoch 16: 540934.0\n",
      "Loss after epoch 17: 534969.0\n",
      "Loss after epoch 18: 515628.0\n",
      "Loss after epoch 19: 525492.0\n",
      "Loss after epoch 20: 525633.0\n",
      "Loss after epoch 21: 525747.0\n",
      "Loss after epoch 22: 532589.0\n",
      "Loss after epoch 23: 549554.0\n",
      "Loss after epoch 24: 530454.0\n",
      "Loss after epoch 25: 516066.0\n",
      "Loss after epoch 26: 527237.0\n",
      "Loss after epoch 27: 490034.0\n",
      "Loss after epoch 28: 430248.0\n",
      "Loss after epoch 29: 425046.0\n",
      "Loss after epoch 30: 437306.0\n",
      "Loss after epoch 31: 452064.0\n",
      "Loss after epoch 32: 460336.0\n",
      "Loss after epoch 33: 436206.0\n",
      "Loss after epoch 34: 442962.0\n",
      "Loss after epoch 35: 433718.0\n",
      "Loss after epoch 36: 436796.0\n",
      "Loss after epoch 37: 433984.0\n",
      "Loss after epoch 38: 435630.0\n",
      "Loss after epoch 39: 434304.0\n",
      "Loss after epoch 40: 424554.0\n",
      "Loss after epoch 41: 431904.0\n",
      "Loss after epoch 42: 449136.0\n",
      "Loss after epoch 43: 420814.0\n",
      "Loss after epoch 44: 429950.0\n",
      "Loss after epoch 45: 445342.0\n",
      "Loss after epoch 46: 431700.0\n",
      "Loss after epoch 47: 420226.0\n",
      "Loss after epoch 48: 420522.0\n",
      "Loss after epoch 49: 420638.0\n",
      "Loss after epoch 50: 435730.0\n",
      "Loss after epoch 51: 411946.0\n",
      "Loss after epoch 52: 424342.0\n",
      "Loss after epoch 53: 408024.0\n",
      "Loss after epoch 54: 419538.0\n",
      "Loss after epoch 55: 420356.0\n",
      "Loss after epoch 56: 420192.0\n",
      "Loss after epoch 57: 417378.0\n",
      "Loss after epoch 58: 423122.0\n",
      "Loss after epoch 59: 421556.0\n",
      "Loss after epoch 60: 406412.0\n",
      "Loss after epoch 61: 412492.0\n",
      "Loss after epoch 62: 402454.0\n",
      "Loss after epoch 63: 431314.0\n",
      "Loss after epoch 64: 415850.0\n",
      "Loss after epoch 65: 407892.0\n",
      "Loss after epoch 66: 399904.0\n",
      "Loss after epoch 67: 240808.0\n",
      "Loss after epoch 68: 234828.0\n",
      "Loss after epoch 69: 236432.0\n",
      "Loss after epoch 70: 227788.0\n",
      "Loss after epoch 71: 240284.0\n",
      "Loss after epoch 72: 227112.0\n",
      "Loss after epoch 73: 228536.0\n",
      "Loss after epoch 74: 227380.0\n",
      "Loss after epoch 75: 223096.0\n",
      "Loss after epoch 76: 221328.0\n",
      "Loss after epoch 77: 226008.0\n",
      "Loss after epoch 78: 222588.0\n",
      "Loss after epoch 79: 215784.0\n",
      "Loss after epoch 80: 218032.0\n",
      "Loss after epoch 81: 209492.0\n",
      "Loss after epoch 82: 205996.0\n",
      "Loss after epoch 83: 203300.0\n",
      "Loss after epoch 84: 204460.0\n",
      "Loss after epoch 85: 206392.0\n",
      "Loss after epoch 86: 196020.0\n",
      "Loss after epoch 87: 202692.0\n",
      "Loss after epoch 88: 196424.0\n",
      "Loss after epoch 89: 185704.0\n",
      "Loss after epoch 90: 190724.0\n",
      "Loss after epoch 91: 189988.0\n",
      "Loss after epoch 92: 178368.0\n",
      "Loss after epoch 93: 178172.0\n",
      "Loss after epoch 94: 182960.0\n",
      "Loss after epoch 95: 180676.0\n",
      "Loss after epoch 96: 175316.0\n",
      "Loss after epoch 97: 172228.0\n",
      "Loss after epoch 98: 168136.0\n",
      "Loss after epoch 99: 166960.0\n"
     ]
    }
   ],
   "source": [
    "rw = BiasedRandomWalk(graph)\n",
    "walks = rw.run(graph.nodes(), n=num_walks, length=walk_length, p=p, q=q)\n",
    "print(f\"Number of random walks for Graph: {len(walks)}\")\n",
    "\n",
    "model = Word2Vec(\n",
    "    walks,\n",
    "    vector_size=dimensions,\n",
    "    window=window_size,\n",
    "    min_count=0,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    cbow_mean=1,\n",
    "    workers=workers,\n",
    "    epochs=num_iter,\n",
    "    compute_loss=True,\n",
    "    callbacks=[callback()]\n",
    ")\n",
    "model.get_latest_training_loss()\n",
    "model.save(f\"../lm-vol/2_28_new_structure_word2vec.model\")\n",
    "model.wv.save(f\"../lm-vol/2_28_new_structure_word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 1151925.25\n",
      "Loss after epoch 1: 915426.125\n",
      "Loss after epoch 2: 767171.125\n",
      "Loss after epoch 3: 754841.5\n",
      "Loss after epoch 4: 665682.5\n",
      "Loss after epoch 5: 597978.0\n",
      "Loss after epoch 6: 627858.5\n",
      "Loss after epoch 7: 634333.5\n",
      "Loss after epoch 8: 620995.5\n",
      "Loss after epoch 9: 582733.5\n",
      "Loss after epoch 10: 545363.0\n",
      "Loss after epoch 11: 606058.5\n",
      "Loss after epoch 12: 517048.0\n",
      "Loss after epoch 13: 551043.0\n",
      "Loss after epoch 14: 553164.0\n",
      "Loss after epoch 15: 503703.0\n",
      "Loss after epoch 16: 529072.0\n",
      "Loss after epoch 17: 557045.0\n",
      "Loss after epoch 18: 500790.0\n",
      "Loss after epoch 19: 529170.0\n",
      "Loss after epoch 20: 564730.0\n",
      "Loss after epoch 21: 549851.0\n",
      "Loss after epoch 22: 596270.0\n",
      "Loss after epoch 23: 538685.0\n",
      "Loss after epoch 24: 552950.0\n",
      "Loss after epoch 25: 543224.0\n",
      "Loss after epoch 26: 588844.0\n",
      "Loss after epoch 27: 487458.0\n",
      "Loss after epoch 28: 458192.0\n",
      "Loss after epoch 29: 467352.0\n",
      "Loss after epoch 30: 467362.0\n",
      "Loss after epoch 31: 470612.0\n",
      "Loss after epoch 32: 472988.0\n",
      "Loss after epoch 33: 485698.0\n",
      "Loss after epoch 34: 491008.0\n",
      "Loss after epoch 35: 484242.0\n",
      "Loss after epoch 36: 467880.0\n",
      "Loss after epoch 37: 488576.0\n",
      "Loss after epoch 38: 477462.0\n",
      "Loss after epoch 39: 495098.0\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    walks,\n",
    "    vector_size=dimensions,\n",
    "    window=window_size,\n",
    "    min_count=0,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    cbow_mean=1,\n",
    "    workers=workers,\n",
    "    epochs=500,\n",
    "    compute_loss=True,\n",
    "    callbacks=[callback()]\n",
    ")\n",
    "model.get_latest_training_loss()\n",
    "model.save(f\"../lm-vol/2_28_new_structure_500_epochs_word2vec.model\")\n",
    "model.wv.save(f\"../lm-vol/2_28_new_structure_500_epochs_word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544792c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings = (model.wv.vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7503a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def operator_hadamard(u, v):\n",
    "    return u * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an edge splitter on the original graph:\n",
    "edge_splitter_test = EdgeSplitter(G)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from graph, and obtain the\n",
    "# reduced graph graph_test with the sampled links removed:\n",
    "graph_test, edges_train, labels_train = edge_splitter_test.train_test_split(\n",
    "    p=0.99, method=\"global\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21412088",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_edges = [operator_hadamard(model.wv[edge[0]], model.wv[edge[1]]) for edge in edges_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(cv=5, max_iter=1000, random_state=0).fit(training_edges, labels_train)\n",
    "clf.score(training_edges, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = []\n",
    "inconclusive = 0\n",
    "for chunk in pd.read_csv('../lm-vol/LANL_test.csv', header=None, chunksize=1000000):\n",
    "    display(chunk)\n",
    "    for index, row in chunk.iterrows():\n",
    "        try: \n",
    "            client_embedding = model.wv[(row[graph_structure[0]])]\n",
    "            ip_embedding = model.wv[(row[graph_structure[1]])]\n",
    "            service_embedding = model.wv[(row[graph_structure[2]])]\n",
    "            client_to_ip_embedding = [operator_hadamard(client_embedding, ip_embedding)]\n",
    "            ip_to_service_embedding = [operator_hadamard(ip_embedding, service_embedding)]\n",
    "            client_to_ip = clf.predict_proba(client_to_ip_embedding)\n",
    "            ip_to_service = clf.predict_proba(ip_to_service_embedding)\n",
    "            anomalies.append([client_to_ip, ip_to_service])\n",
    "            \n",
    "        except: \n",
    "            anomalies.append('')\n",
    "            inconclusive += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36405191",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_to_ip_pred = []\n",
    "ip_to_service_pred = []\n",
    "for anomaly in anomalies:\n",
    "    if anomaly != '':\n",
    "        client_to_ip_pred.append(anomaly[0][0][1])\n",
    "        ip_to_service_pred.append(anomaly[1][0][1])\n",
    "    else:\n",
    "        client_to_ip_pred.append(None)\n",
    "        ip_to_service_pred.append(None)\n",
    "test_df['client_to_ip_pred'] = client_to_ip_pred\n",
    "test_df['ip_to_service_pred'] = ip_to_service_pred\n",
    "print(inconclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb46af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bde14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_df= pd.DataFrame(anomalies)\n",
    "anomalies_df.to_csv('../lm-vol/anomalies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies2 = []\n",
    "inconclusive2 = 0\n",
    "for chunk in pd.read_csv('../lm-vol/LANL_test.csv', header=None, chunksize=1000000):\n",
    "    display(chunk)\n",
    "    for index, row in chunk.iterrows():\n",
    "        try: \n",
    "            client_embedding = model.wv[(row[graph_structure[0]])]\n",
    "            ip_embedding = model.wv[(row[graph_structure[1]])]\n",
    "            service_embedding = model.wv[(row[graph_structure[2]])]\n",
    "            client_to_ip_embedding = [operator_hadamard(client_embedding, ip_embedding)]\n",
    "            ip_to_service_embedding = [operator_hadamard(ip_embedding, service_embedding)]\n",
    "            client_to_ip = clf.predict_proba(client_to_ip_embedding)\n",
    "            ip_to_service = clf.predict_proba(ip_to_service_embedding)\n",
    "            anomalies2.append([client_to_ip, ip_to_service])\n",
    "            \n",
    "        except: \n",
    "            anomalies2.append('')\n",
    "            inconclusive += 1\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
